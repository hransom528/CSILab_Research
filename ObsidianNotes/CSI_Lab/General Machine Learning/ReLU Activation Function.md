The **rectifier** or **Rectified Linear Unit** (ReLU) activation function is a common [[Activation Function]] that can be used in [[Neural Network]].

The ReLU function is defined as the positive part of its argument:
$$
f(x) = x^+ = max(0, x) = \frac{x+|x|}{2}
$$